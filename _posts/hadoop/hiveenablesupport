Spark enableHiveSupport(Hive metaStore)
Skip to end of metadata
김규훈 매니저님이 작성, 2022 2월 10에 최종 변경Go to start of metadata
spark에서 Hive table을 다룰 수 있도록 하기 위해서 'enableHiveSupport()'를 주로 사용하는데요. 

spark = SparkSession.builder.enableHiveSupport().config(conf=conf).getOrCreate()
 
df = spark.sql("""
SELECT *
FROM vcrm_2918888.test1
""")


'enableHiveSupport()'의 의미가 무엇인지 생각해보지 않았던 것 같아서 한번 조사해보았습니다.

이에 대해 알기 위해서는 Hive Table에 대해서 먼저 알아야합니다.

예를 들어 vcrm_2918888.test_1 이라는 테이블이 있다고 가정하면, 해당 테이블은 HDFS 상에서 '/usre/hive/warehouse/vcrm_2918888.db/test_1' 경로에 저장되어 있습니다.

이를 TABLE LOACTION이라고 합니다.

나아가, PARTITIONED TABLE의 경우에는 해당 LOCATION 하위에 PARTITION DIR이 더 존재합니다.

예시1	vcrm_2918888.test_1	/user/hive/warehouse/vcrm_2918888.db/test_1	
예시2	default.test_2	/user/hive/warehouse/default.db/test_2	
파티션	vcrm_2918888.vc2_hmc_ev_ne_rtlog	/user/hive/warehouse/vcrm_2918888.db/vc2_hmc_ev_ne_rtlog	
/user/hive/warehouse/vcrm_2918888.db/vc2_hmc_ev_ne_rtlog/p_date=20210101

~

/user/hive/warehouse/vcrm_2918888.db/vc2_hmc_ev_ne_rtlog/p_date=20211231

우리가 Hive에 쿼리를 작성하여 테이블을 조회하는 것은 사실 HDFS에서 LOCATION에 존재하는 파일을 읽는 것입니다.

따라서, 우리가 아래 쿼리를 실행하면

SELECT *
FROM vcrm_2918888.test_1
Hive는 vcrm_2918888.test_1에 해당하는 LOCATION에서 파일을 읽어 결과를 내놓습니다

[2918888@pb01gn001 ~]$ hdfs dfs -ls /user/hive/warehouse/vcrm_2918888.db/test_1
# parquet format 파일들 존재할 것입니다
그렇다면 TABLE의 LOCATION, PARTITION 정보 즉 hive metastore는 어디에 저장하고 있을까요? hive-stie.xml에서 hive metastore의 저장 위치를 정의합니다.

hive metastore는 임베디드, 로컬, 원격 셋 중에 하나로 구성할 수 있는데, 회사는 원격 메타스토어를 사용하고 있습니다. (https://wikidocs.net/28353)

[2918888@pb01gn001 ~]$ cat /etc/hive/3.0.1.0-187/0/hive-site.xml
...
<property>
      <name>hive.metastore.uris</name>
      <value>thrift://pb01mn001.pkr0c11.hcloud.io:9083</value>
 </property> 
...
...
"pb01mn001.pkr0c11.hcloud.io:9083 "에 hive metastore가 저장되어있음을 확인할 수 있습니다. 즉, 해당 경로에 hive table들의 location, partition 정보와 같은 메타 데이터들이 저장되어 있습니다. 

spark의 enableHiveSupport()는 pb01mn001.pkr0c11.hcloud.io:9083에 접근하여 hive metastore를 사용하겠다 라는 의미입니다.

deafult로 hive-site.xml에서 정의된 값을 사용하고, spark.sql.warehouse.dir를 입력하면 해당 경로에서 metastore 경로를 검색합니다. 

When working with Hive, one must instantiate SparkSession with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. Users who do not have an existing Hive deployment can still enable Hive support. When not configured by the hive-site.xml, the context automatically creates metastore_db in the current directory and creates a directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse in the current directory that the Spark application is started. Note that the hive.metastore.warehouse.dir property in hive-site.xml is deprecated since Spark 2.0.0. Instead, use spark.sql.warehouse.dir to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application.

(https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html)

그렇기 때문에 enableHiveSupport()를 선언한 sparkSession에서 spark.sql을 이용하여 쿼리를 실행하여도, 해당 테이블의 경로(location)을 찾아서 테이블에 해당하는 파일(데이터)를 읽어들여올 수 있습니다.

따라서 아래 두 방법은 기본적으로 같은 동작을 수행합니다.

df1= spark.sql("""
SELECT *
FROM vcrm_2918888.test1
""")
# vcrm_2918888.test1 테이블의 LOCATION은 /user/hive/warehouse/vcrm_2918888.db/test1 입니다.
 
df2 = spark.read.parquet("/user/hive/warehouse/vcrm_2918888.db/test1")
